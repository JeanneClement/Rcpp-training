---
title: "Rcpp training"
output:
  bookdown::html_document2:
    #base_format: rmarkdown::html_vignette
    #highlight: tango
    number_sections: true
    toc: true
    #toc_float: true
    fig_caption: yes
link-citations: yes
pkgdown:
  as_is: true
vignette: >
 %\VignetteEngine{knitr::rmarkdown}
 %\VignetteIndexEntry{Rcpp training}
 %\VignetteEncoding{UTF-8}
urlcolor: Maroon
---

```{r options, include=FALSE}
library(knitr)
library(kableExtra)
library(pkgdown)
library(dplyr)
opts_chunk$set(echo=TRUE, cache=TRUE,
               #results="hide", 
               warning=FALSE,
               message=FALSE, highlight=TRUE,
               fig.show="hide", size="small",
               fig.align="center",
               tidy=FALSE)
options(knitr.kable.NA="-")
```

# RcppGSL example

## C++ code
```{Rcpp RcppGSL-my_rnorm}
#include <Rcpp.h>
#include <gsl/gsl_rng.h>
#include <gsl/gsl_randist.h>
using namespace Rcpp;
// [[Rcpp::depends(RcppGSL)]]
// [[Rcpp::export]]
Rcpp::NumericVector my_rnorm(int nsamp, double mu,
                             double sigma) {
	gsl_rng *s = gsl_rng_alloc(gsl_rng_mt19937); // Random seed
	Rcpp::NumericVector beta(nsamp);
	for (int i = 0; i < nsamp; i++) {
		beta[i] = mu + gsl_ran_gaussian(s, sigma); // Random draw
	}
	return beta;
}
```
\vspace{0.3cm}

## R code
```{r r-my_rnorm, fig.show="asis", out.width="70%"}
library(Rcpp)
library(RcppGSL)
beta <- my_rnorm(100, 5, 2)
par(cex=2)
hist(beta)
```

# RcppArmadillo example

## C++ code
```{Rcpp RcppArma-example}
#include <RcppArmadillo.h>
// [[Rcpp::depends(RcppArmadillo)]]
// [[Rcpp::export]]
arma::mat arma_scale(const arma::mat& X) {
    int n = X.n_rows, k = X.n_cols;
    arma::rowvec col_means = arma::mean(X,0); // means 
    arma::rowvec col_sd  = arma::stddev(X,0); // standard deviations
    arma::mat X_scaled(n,k);
    for (int  p= 0; p < k; p++) {
    X_scaled.col(p) = (X.col(p)-col_means(p))/col_sd(p);
    }
    return X_scaled;
}
```

## R code
```{r r-RcppArma-example}
library(Rcpp)
library(RcppArmadillo)

# Center and reduce a matrix 
X <- matrix(rnorm(50),ncol=5)
X_scaled <- arma_scale(X)
colMeans(X_scaled)
var(X_scaled)
```


# Distance computation 

## Mathematical definition

The Euclidean distance between two points whose coordinates are $A=(x_A, y_A)$ and $B=(x_B,y_B)$ is given by $$\sqrt{(x_B-x_A)^2 + (y_B-y_A)^2}.$$

## C++ function 
(in file `Code/arma_distmat.cpp`)
```{Rcpp arma_distmat}
#include <RcppArmadillo.h>
// [[Rcpp::depends(RcppArmadillo)]]
using namespace Rcpp;
using namespace arma;

// [[Rcpp::export]]
arma::mat arma_distmat(const arma::mat& x) { 
  int np = x.n_rows;
  arma::mat distmat; distmat.zeros(np, np);
  for (int i = 0; i < np; i++) {
    arma::vec p0 = x.row(i).t();
    for (int j = i + 1; j < np; j++) {
      arma::vec p1 = x.row(j).t();
      arma::vec diff = p0 - p1;
      double squared_diff = as_scalar(diff.t() * diff);
      distmat(j, i) = distmat(i, j) = sqrt(squared_diff);
    }
  }
  return distmat;
}
```

## R function
```{r R_distmat}
R_distmat <- function(X){
  np <- nrow(X)
  distmat <- matrix(0,nrow=np,ncol=np)
  for (i in 1:(np-1)) {
    p0 <- X[i,]
    for (j in (i+1):np){
    p1 <- X[j,]
    diff <- p0-p1
    squared_diff <- t(diff)%*% diff
    distmat[j,i] <- distmat[i,j] <- sqrt(squared_diff)
    }
  }
 return(distmat)
}
```

## Comparison of compilation times
```{r dist-benchmark}
library(rdist)
# Data simulation 
coords <- matrix(runif(2000,0,100),ncol=2)
# Call of C++ function
Rcpp::sourceCpp("Code/arma_distmat.cpp")
# Benchmark
library(rbenchmark)
Benchmark <- benchmark(
  "arma_distmat" = {arma_distmat(coords)},
  "cdist" = {cdist(coords,coords)},
  "R_distmat" = {R_distmat(coords)},
  replications=30,
  columns = c("test", "elapsed", "relative"))
```

```{r dist-benchmark-results, echo=F}
library(dplyr)
knitr::kable(Benchmark,digits=1, booktabs=TRUE) %>%
		kableExtra::kable_styling(latex_options=c("HOLD_position","striped"),
		                          full_width=FALSE,position = "left")
```

## Representation of results 
\vspace{0.5cm}

**1000 points dispersed all over the space : **
\vspace{0.2cm}
```{r points-representation, echo=F,  fig.show="asis", out.width="60%",out.height="60%", fig.align="left"}
# Spatial representation of points  
coords <- matrix(runif(2000,0,100),ncol=2)
plot(coords[,1],coords[,2],pch=4,xlab="x", ylab="y", main="Spatial repartition of points", col="forestgreen")
```
\vspace{0.2cm}
**Coordinates of 5 points**
\vspace{0.1cm}
```{r coordinates, echo=F}
knitr::kable(coords[1:5,],col.names = c(" x "," y "), digits=1, booktabs=TRUE) %>%
		kableExtra::kable_styling(latex_options=c("HOLD_position","striped"),
		                          full_width=FALSE,position = "left")
```
\vspace{0.2cm}
**Distance matrix for this points : **
\vspace{0.1cm}
```{r distance-matrix, echo=F}
distmat <- arma_distmat(coords)
knitr::kable(distmat[1:5,1:5], digits=1, booktabs=TRUE) %>%
		kableExtra::kable_styling(latex_options=c("HOLD_position","striped"),
		                          full_width=FALSE,position = "left")
```

# Simple linear regression 

## Mathematical definition 
\vspace{0.2cm} 
We have observations of a response variable available for $n$ individuals $Y=(y_i)_{i=1,\ldots,n}$   

and $p$ explanatory variables $(X_1,\ldots,X_p)$ such as   $X_1=(X_{11},\ldots,X_{i1},\ldots,X_{n1})'$.  

\vspace{1cm}
We want to estimate coefficients of the linear regression $\beta=(\beta_0,\beta_1,\ldots,\beta_p)$ such as : 
$$ y_i = \beta_0 + \beta_1X_{i1}+\ldots+\beta_pX_{ip}+\epsilon_i,$$
where $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$ for $i=1,\ldots,n$.   

\vspace{0.2cm}
Then $y_i \sim \mathcal{N}(\beta_0 + \beta_1X_{i1}+\ldots+\beta_pX_{ip}, \ \sigma^2)$.  

\vspace{0.4cm}

This gives in matrix writing :  

$$Y = X\beta + \epsilon $$ 
where $X=(\mathbb{1}_n,X_1,\ldots,X_p)$ and $\epsilon=(\epsilon_i)_{i=1,\ldots,n}$.  
\vspace{1cm}

According to the Ordinary Least Squares (OLS) method, $\beta$ is estimated by :
$$\widehat{\beta}=\left(X'X\right)^{-1}X'y.$$
\vspace{0.2cm}
We define the residuals  $$\widehat{\epsilon} = Y - X\widehat{\beta}=Y-\widehat{Y}.$$
\vspace{0.2cm}
Then residual variance is given by $$\widehat{\sigma^2}=\dfrac{\sum\limits_{i=1}^n \widehat{\epsilon_i}^2}{n-p-1}.$$  
\vspace{0.2cm}
Finally the varianceâ€“covariance matrix of coefficients is estimated by $$V_{\widehat{\beta}}=\widehat{\sigma^2}\left(X'X\right)^{-1}.$$
\vspace{1cm}

## C++ function
```{Rcpp fastLm}
#include <RcppArmadillo.h>
// [[Rcpp::depends(RcppArmadillo)]]
// [[Rcpp::export]]
Rcpp::List arma_fastLm(const arma::mat& X, const arma::colvec& y) {
    int n = X.n_rows;
    int p = X.n_cols;
        
    arma::colvec coef = arma::inv(X.t()*X)*X.t()*y;    // fit model y ~ X
    arma::colvec res  = y - X*coef;           // residuals
    // std.errors of coefficients
    double s2 = arma::sum(res.t()*res)/(n-p);
    arma::colvec std_err = arma::sqrt(s2*arma::diagvec(arma::inv(X.t()*X)));
    return Rcpp::List::create(Rcpp::Named("coefficients") = coef,
                        Rcpp::Named("stderr") = std_err,
                        Rcpp::Named("residuals")  = res,
                        Rcpp::Named("sigma2")  = s2,
                        Rcpp::Named("df.residual")  = n - p);
}
```

## R function
```{r r-fastLm}
R_fastLm <- function(X, y) {
n <- nrow(X)
p <- ncol(X)

# fit model y ~ X       
coef <-solve(t(X) %*% X) %*% t(X) %*% y
# residuals
res <- y - X %*% coef
# std.errors of coefficients
s2 = sum(t(res) %*% res)/(n - p);
std_err = sqrt(s2 %*% diag(solve(t(X)%*%X)))
return(list("coefficients" = coef,
            "stderr" = std_err,
            "residuals" = res,
            "sigma2" = s2,
            "df.residual" = n - p))
}
```

## Comparison of compilation times
```{r lin-benchmark}
library(datasets)
# Trees data-set
y <- log(trees$Volume)
X <- cbind(1, log(trees$Girth), log(trees$Height))
# Call C++ function
Rcpp::sourceCpp("Code/arma_fastLm.cpp")
# Benchmark
library(rbenchmark)
Benchmark <- benchmark(
  "arma_fastLm" = {arma_fastLm(X,y)},
  "R_fastLm" = {R_fastLm(X,y)},
  replications=100,
  columns = c("test", "elapsed", "relative"))
```

```{r lin-benchmark-results, echo=F}
knitr::kable(Benchmark,digits=4, booktabs=TRUE) %>%
		kableExtra::kable_styling(latex_options=c("HOLD_position","striped"),
		                          full_width=FALSE,position = "left")
```

## Representation of results 
```{r lin-results,fig.show="asis", out.width="50%",out.height="50%"}
# Trees data-set
y <- log(trees$Volume)
X <- cbind(1, log(trees$Girth), log(trees$Height))
# fit model y ~ X
arma_mod <- arma_fastLm(X,y)
R_mod <- R_fastLm(X,y)
# Comparison of residuals obtained with R and C++ functions
plot(arma_mod$residuals,R_mod$residuals,xlab="residuals with C++",ylab="residuals with R",pch=4, main="Residuals")
abline(a=0,b=1, col="red")
# Representation of the linear regression
plot(X%*%arma_mod$coefficients,y,xlab="Xbeta",ylab="y",pch=4, main ="Linear regression")
abline(a=0,b=1, col="red")
```

# Loglikelihood computation 

## Mathematical definition
\vspace{0.2cm}
The likelihood function expresses the plausibilities of different parameter values for a given sample of data.  
The maximum of this function, if it exists, correspond to the combination of model parameter values that maximize the probability of drawing the sample actually obtained.  
\vspace{0.2cm}
The likelihood corresponding to the previous simple linear model is given by :
$$\begin{aligned}
L(\beta,\sigma^2) &= \mathbb{P}(y_i \ | \ \sigma, \beta) \\
&=\prod \limits_{i=1}^n \frac{1}{\sigma \sqrt{2\pi}}\exp{\left(-\frac{1}{2\sigma^2}(y_i-\beta_0-X_{i1}\beta_1-\ldots-X_{ip}\beta_p)^2 \right)}
\end{aligned}$$

Then the log-likelihood is :
$$\begin{aligned}
l(\beta,\sigma^2) &= \log{\left(L(\beta,\sigma^2)\right)} \\
&= \sum \limits_{i=1}^n \ \log \left(\frac{1}{\sigma \sqrt{2\pi}}\right) -\frac{1}{2\sigma^2}(y_i-\beta_0-X_{i1}\beta_1-\ldots-X_{ip}\beta_p)^2\\
&= -n\log(\sigma)-n\frac{\log(2\pi)}{2} -\frac{1}{2\sigma^2} \sum \limits_{i=1}^n (y_i-\beta_0-X_{i1}\beta_1-\ldots-X_{ip}\beta_p)^2
\end{aligned}$$

Implement a function in C++ using RcppArmadillo to compute the log-likelihood corresponding to each of the following models fitted on the trees dataset used previously :  

- Model 1 : $y_i = \beta_0 + \beta_1G_i + \epsilon_i$

- Model 2 : $y_i = \beta_0 + \beta_1H_i + \epsilon_i $

- Model 3 : $y_i = \beta_0 + \beta_1G_i + \beta_2H_i + \epsilon_i$

## C++ function 
```{Rcpp arma_logL}
#include <RcppArmadillo.h>
// [[Rcpp::depends(RcppArmadillo)]]
// [[Rcpp::export]]
double arma_logL(arma::mat X, arma::vec y, arma::vec beta, double sigma2) {
  int n = y.n_elem;
  int p = X.n_cols;
  double logL = 0;
  for(int i = 0; i < n; i++) {
      double Xbeta_part = 0;
    for(int j = 0; j < p; j++) {
      Xbeta_part += X(i,j)*beta(j);
    }
    logL += R::dnorm(y[i], Xbeta_part, std::sqrt(sigma2), 1);
  }
  return logL;
}
```

## R function 
```{r R_logL}
R_logL <- function(X,y,beta,sigma2) {
  n <- length(y)
  p <- ncol(X)
  
  logL <- 0
  for (i in 1:n) {
    Xbeta_part <- 0
    for(j in 1:p) {
      Xbeta_part <- Xbeta_part + X[i,j]*beta[j]
    }
    logL <- logL + dnorm(x=y[i], mean=Xbeta_part, sd=sqrt(sigma2), log=T)
  }
  return(logL)
}
```

## Comparison of compilation times
```{r logL-benchmark}
# Trees data-set
y <- log(trees$Volume)
X <- cbind(1, log(trees$Girth), log(trees$Height))
# fit model y ~ X
arma_mod <- arma_fastLm(X,y)
R_mod <- R_fastLm(X,y)
# Call C++ function
Rcpp::sourceCpp("Code/arma_logL.cpp")
# Benchmark
Benchmark <- benchmark(
  "arma_logL" = {arma_logL(X,y,arma_mod$coefficients,arma_mod$sigma2)},
  "R_logL" = {R_logL(X,y,R_mod$coefficients,R_mod$sigma2)},
  replications=100,
  columns = c("test", "elapsed", "relative"))
```

```{r logL-benchmark-results, echo=F}
library(dplyr)
knitr::kable(Benchmark,digits=3, booktabs=TRUE) %>%
		kableExtra::kable_styling(latex_options=c("HOLD_position","striped"),
		                          full_width=FALSE, position = "left")
```

## Representation of results 

```{r logL-results, echo=F}
# Trees data-set
y <- log(trees$Volume)
# fit models y ~ X
## model 1
X1 <- cbind(1, log(trees$Girth))
arma_mod1 <- arma_fastLm(X1,y)
R_mod1 <- R_fastLm(X1,y)
arma_logL1 <- arma_logL(X1,y,arma_mod1$coefficients,arma_mod1$sigma2)
R_logL1 <- R_logL(X1,y,R_mod1$coefficients, R_mod1$sigma2)
## model 2
X2 <- cbind(1, log(trees$Height))
arma_mod2 <- arma_fastLm(X2,y)
R_mod2 <- R_fastLm(X2,y)
arma_logL2 <- arma_logL(X2,y,arma_mod2$coefficients,arma_mod2$sigma2)
R_logL2 <- R_logL(X2,y,R_mod2$coefficients,R_mod2$sigma2)
## model 3
X3 <- cbind(1, log(trees$Girth), log(trees$Height))
arma_mod3 <- arma_fastLm(X3,y)
R_mod3 <- R_fastLm(X3,y)
arma_logL3 <- arma_logL(X3,y,arma_mod3$coefficients,arma_mod3$sigma2)
R_logL3 <- R_logL(X3,y,R_mod3$coefficients,R_mod3$sigma2)
logL <- data.frame(arma_logL=c(arma_logL1,arma_logL2,arma_logL3), R_logL=c(R_logL1,R_logL2,R_logL3))
rownames(logL) <- c("model 1","model 2","model 3")
knitr::kable(logL,digits=3, booktabs=TRUE, row.names = T) %>%
		kableExtra::kable_styling(latex_options=c("HOLD_position","striped"),
		                          full_width=FALSE,position = "left")
```

